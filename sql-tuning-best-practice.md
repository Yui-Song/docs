---
title: SQL Tuning Best Practice
summary: Learn how to do SQL tuning in TiDB

---

# Introduction to SQL Tuning

SQL tuning is a critical aspect of optimizing database system performance. It involves a systematic approach to improve the efficiency of SQL queries. The process typically consists of three key steps:

1. Identify high-impact SQL statements:
   - Review SQL execution history to find statements that consume a large portion of system resources or contribute significantly to the application workload.
   - Use monitoring tools and performance metrics to pinpoint these resource-intensive queries.

2. Analyze execution plans:
   - Examine the execution plans generated by the query optimizer for the identified statements.
   - Verify that these plans are reasonably efficient and utilize appropriate indexes and join methods.

3. Implement optimizations:
   - Apply corrective actions to improve poorly performing SQL statements.
   - This may include rewriting queries, adding or modifying indexes, updating statistics, or adjusting database parameters.

These steps are iteratively repeated until:
- The system performance meets the desired targets
- No further improvements can be made to the remaining statements.

It's important to note that SQL tuning is an ongoing process. As your data volumes grow and query patterns evolve, you should:
- Regularly monitor query performance
- Re-evaluate your optimization strategies
- Adapt your approach to address new performance challenges

By consistently applying these practices, you can ensure that your database maintains optimal performance over time.

# Goals for Tuning

The primary objectives of SQL tuning are to:

1. Reduce response time for end users
2. Minimize resource consumption for processing workloads

These goals can be achieved through various strategies:

## Optimize Query Execution

SQL tuning often involves finding more efficient ways to process the same workload without changing the query's functionality. This can be done by:

1. Improving execution plans:
   - Analyze and modify query structures to enable more efficient processing
   - Utilize appropriate indexes to reduce data access and processing time
   - Enable TiFlash for analytical queries on large volumes of data and Leverage MPP (Massively Parallel Processing) engine for complex aggregations and joins

2. Enhancing data access methods:
   - Use covering indexes to satisfy queries directly from the index, avoiding table access
   - Implement partitioning strategies to limit data scans to relevant partitions

Examples:

- Creating an index for frequently queried columns can significantly reduce resource usage, especially for queries that access a small percentage of table data.
- Utilizing index-only scans for queries that return a limited number of sorted results can avoid full table scans and sorting operations.

## Balance Workload Distribution

In TiDB's distributed architecture, ensuring even workload distribution across multiple TiKV nodes is crucial for optimal performance:

- Prevent hotspots: Design schemas and queries to avoid concentrated read or write operations on specific nodes
- Utilize parallel processing:
   - Design queries to take advantage of TiDB's distributed execution capabilities
   - Enable and tune parallel execution settings for resource-intensive operations

By implementing these strategies, you can ensure that your TiDB cluster efficiently utilizes all available resources and avoids bottlenecks caused by uneven workload distribution or serialization on individual TiKV nodes.

# Identifying High-Load SQL
The most efficient way to identify Resource-Intensive SQL is using TiDB Dashboard, There are other tools like views and logs available as well.

## Monitoring SQL Statements by Using TiDB Dashboard
### SQL Statements Panel 
In TiDB Dashboard, navigate to SQL Statements panel, which helps us identify the following:

1. The SQL statement with the highest total latency is the one that takes the longest time to execute out of multiple executions of the same SQL statement. 
2. It can also display the number of times each SQL statement has been executed cumulatively, allowing us to find the SQL statement with the highest execution frequency.
3. Clicking on each SQL statement allows us to delve deeper into the EXPLAIN ANALYZE results.

SQL statements are normalized as templates, where literals and bind variables are replaced by '?'. This normalization and sorting process allows you to quickly pinpoint the most resource-intensive queries that may require optimization.
![sql-statements-default](/media/sql-tuning/sql-statements-default.png)


### Slow Queries Panel Default Display
In the TiDB Dashboard, we can find the Slow Query panel, which displays all SQL statements whose execution time exceeds the time threshold set by the system variable tidb_slow_log_threshold (default 300 milliseconds). 

On Slow Queries panel, we can find:

1. The slowest SQL queries.
2. The SQL query that reads the most data from TiKV.
3. The EXPLAIN ANALYZE output from drilling down the SQL statement by clicking it.
4. Please note that on the Slow Queries panel, we cannot get the frequency of the SQL statement execution. Once the execution elapsed time exceeds tidb_slow_log_threshold for single instance, the query is then listed on the Slow Queries panel.
![slow-query-default](/media/sql-tuning/slow-query-default.png)

## Other Tools for Identifying Top SQL

In addition to TiDB Dashboard, there are several other tools available to identify resource-intensive SQL queries:

Each tool offers unique insights and can be valuable for different analysis scenarios. Using a combination of these tools allows for comprehensive SQL performance monitoring and optimization.
- [slow query log](https://docs.pingcap.com/tidb/stable/identify-slow-queries)
- [statements_summary view](https://docs.pingcap.com/tidb/stable/statement-summary-tables#statements_summary)
- [top sql feature](https://docs.pingcap.com/tidb/stable/top-sql)
- [expensive queries in tidb log](https://docs.pingcap.com/tidb/stable/identify-expensive-queries)
- [cluster_processlist view](https://docs.pingcap.com/tidb/stable/information-schema-processlist#cluster_processlist)


## Gathering Data on the SQL Identified

For the top SQL statements identified, you can use [PLAN REPLAYER](https://docs.pingcap.com/tidb/stable/sql-plan-replayer) to capture and save the on-site information of a TiDB cluster. This tool allows you to recreate the execution environment for further analysis. The syntax for exporting SQL information is as follows:

```SQL
PLAN REPLAYER DUMP EXPLAIN [ANALYZE] [WITH STATS AS OF TIMESTAMP expression] sql-statement;
```

Use EXPLAIN ANALYZE whenever possible, as it captures actual execution information in addition to the execution plan. This provides more accurate insights into query performance.

# SQL Tuning Guide

This guide focuses on providing actionable advice for beginners looking to optimize their SQL queries in TiDB. By following these best practices, you can ensure better query performance and SQL Tuning. We'll cover below topic:
- Query Processing Workflow
- Optimizer Fundamentals
- Statistics Management
- How TiDB build A Execution Plan
- Understand Execution Plan
- Real-World Use Case
  - Bad Plan Debug
  - Index Strategy in TiDB
  - Partition table: local index vs global index
  - Index Full Scan is fater than Table Full Scan

## Query Processing Workflow
the client sends a SQL statement to the protocol layer of TiDB server. The protocol layer is responsible for handling the connection between TiDB Server and the client, receiving SQL statement from the client, and returning data to the client.

To the right of the protocol layer is the Optimizer layer of TiDB Server, which is responsible for processing SQL statements. The process is as follows:

1. SQL statement arrives at the SQL optimizer through the protocol layer and is first parsed into an Abstract Syntax Tree (AST).
2. Pre-Process is primarily for Point Get. If it is a Point Get, the following-up optimization processes can be skipped, the next step jumps to the SQL Executor.
3. After confirming that it is not a Point Get, the AST goes to the Logical Transformation, which rewrites the SQL logically based on certain rules.
4. The AST that has gone through Logical Transformation will undergo Cost-Based Optimization.
5. During Cost-Based Optimization, the optimizer considers statistics to determine how to select specific operators and finally generates a physically executable physical execution plan.
6. The generated physical execution plan is sent to the SQL Executor of the TiDB database for execution.
7. Unlike traditional databases, TiDB databases need to push down the execution plan to different TiKV for reading and writing data.

![workflow](/media/sql-tuning/workflow.png)

## Optimizer Fundamentals

TiDB uses a cost-based optimizer (CBO) to determine the most efficient execution plan for a SQL statement. This optimizer evaluates different execution strategies and chooses the one with the lowest estimated cost. The cost is influenced by factors such as:

- SQL
- Schema Design
- Statistics
  - Table
  - Index
  - Column

Based on the input, The cost model will produce the execution plan, which includes the details how the system execute the sql, including 
- Access Method
- Join Method
- Join Order

The optimizer is as good as the information it receives. Therefore, ensuring up-to-date statistics and well-designed indexes is critical.

## Statistics Management

Statistics are essential to the TiDB optimizer. TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement.

statistics is generally divided into two levels: table level and column level. 
- For table-level statistics, it includes the total number of rows in the table and the number of rows that have been modified since the last collection of statistics. 
- The column-level statistics information is more abundant, including histograms, Count-Min Sketch, Top-N (values or indexes with the highest occurrences), distribution and quantity of different values, and the number of null values, and so on.

To ensure the statistics are healthy and representative, you can use the following commands:

1. `SHOW STATS_META`: This command provides metadata about table statistics.
2. `SHOW STATS_HEALTHY`: This command shows the health status of table statistics.

For example, you can use:

```SQL
tidb> SHOW STATS_META WHERE table_name='T2'\G;
*************************** 1. row ***************************
 Db_name: test
 Table_name: T2
Partition_name:
 Update_time: 2023-05-11 02:16:50
 Modify_count: 20000
 Row_count: 20000
1 row in set (0.03 sec)

tidb> SHOW STATS_HEALTHY WHERE table_name='T2'\G;
*************************** 1. row ***************************
    Db_name: test
  Table_name: T2
Partition_name:
    Healthy: 0
1 row in set (0.00 sec)

```

In TiDB database, there are two ways to collect statistics: automatic collection and manual collection. In most case, the auto collection job works fine. Automatic collection is actually triggered when certain conditions are met for a table, and TiDB will automatically collect statistics. We commonly use three triggering conditions, which are: ratio, start_time and end_time.
tidb_auto_analyze_ratio: The healthiness trigger
tidb_auto_analyze_start_time and tidb_auto_analyze_end_time: The allowed job window
```SQL
mysql> show variables like '%auto_analyze%';
+-----------------------------------------+-------------+
| Variable_name                           | Value       |
+-----------------------------------------+-------------+
| tidb_auto_analyze_ratio                 | 0.5         |
| tidb_auto_analyze_start_time            | 00:00 +0000 |
| tidb_auto_analyze_end_time              | 23:59 +0000 |
+-----------------------------------------+-------------+
```

In cases where automatic collection doesn't meet your needs, you can manually collect statistics using the `ANALYZE TABLE table_name` statement. This allows you to:

1. Adjust the sample rate for more accurate or faster analysis
2. Increase the number of top-N values collected
3. Gather statistics for specific columns only

It's important to note that after manual collection, subsequent automatic gathering jobs will inherit the new parameters. This means that any customizations you've made during manual collection will be carried forward in future automatic analyses.

Another common scenario is locking table statistics. This is useful when:
1. The statistics on the table are already representative of the data.
2. The table is very large and statistics collection is time-consuming.
3. You want to maintain statistics only during specific time windows.

To lock the statistics for a table, you can use the following command `LOCK STATS table_name`.

for more detail about statistics, please refer to [statistics](https://docs.pingcap.com/tidb/stable/statistics).

## How TiDB build A Execution Plan
An SQL statement undergoes optimization primarily in the optimizer through three stages:
- Pre-Processing
- Logical Transformation
- Cost-based Optimization

### Pre-Processing
The main actions in the pre-processing stage it to determine if the SQL statement can be executed by using Point_Get or Batch_Point_Get.

Point_Get or Batch_Point_Get is to get 1 or 0 or many row only by using the TiKV key, the explicit or implicit (_tidb_rowid) primary key. For example, when id column is the primary key of a clustered index table, Point_Get is used to get the particular row. If a plan is identified as Point_Get, optimizer will skip the logical transformation and cost-based optimization.

```SQL
SELECT id, name FROM emp WHERE id = 901; 
```

### Logical Transformation

The purpose of logical Transformation is to optimize the execution of statements based on the characteristics of SELECT list, WHERE predicates, and other predicates in SQL queries. It generates a logical execution plan to annotate and rewrite the query. This logical plan is then passed to the Cost-Based Optimization. The optimization rules include column pruning，partition pruning，eliminate Max/Min, eliminate outer join, join reorder, predicates push-down，subquery rewrite，derive TopN from window functions，and de-correlation of correlated Subquery. Since this step is fully automated by the query optimizer, it usually does not require manual adjustments.

More Detail for Logical Transformation: https://docs.pingcap.com/tidb/stable/sql-logical-optimization.

### Cost-Based Optimization

TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement, and associates a cost with each plan step. The Cost-Based Optimization estimates the cost of each available plan choice, including index accesses and the sequence of table joins, and produces a cost for each available plan. The optimizer then picks the execution plan with the lowest overall cost.

The below figure illustrates the various data access paths and row set operations that cost-based optimization can consider to develop the optimal execution plan. Furthermore, during the logical transformation stage, the query has already been rewritten for predicate push-down. In the cost-based optimization stage, TiKV expression push-down is further implemented when possible at TiKV layer. 

Furthermore, confirming the algorithm for certain SQL operations, such as aggregation, join, and sorting, is essential. For instance, the aggregation operator may utilize either HASH_AGG or STREAM_AGG, while the join operator can select from HASH JOIN, MERGE JOIN, or INDEX JOIN. Likewise, various options are available for the sorting operator.

![cost-based-optimization](/media/sql-tuning/cost-based-optimization.png)

## Understanding Execution Plans

The execution plan represents the steps TiDB will follow to execute a SQL query. In this section, we will learn how to display and read the execution plan.

### Generating and Displaying Execution Plans

Beside access the execution plan information through TiDB Dashboard, TiDB provides a `EXPLAIN` statement to display the execution plan for a SQL query. Here's an example of using `EXPLAIN`:
- id: Operator name and the step unique identifier
- estRows: Estimated number of rows from the particular step
- task: The executor of the step
- access object: The object where the row sources are located
- operator info: Extended information about the operator regarding the step

```SQL
tidb> EXPLAIN SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';

+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| id                       | estRows     | task         | access object     | operator info                                                                                      |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| StreamAgg_20             | 1.00        | root         |                   | funcs:count(Column#13)->Column#11                                                                  |
| └─TableReader_21         | 1.00        | root         |                   | data:StreamAgg_9                                                                                   |
|   └─StreamAgg_9          | 1.00        | cop[tikv]    |                   | funcs:count(1)->Column#13                                                                          |
|     └─Selection_19       | 250.00      | cop[tikv]    |                   | ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000) |
|       └─TableFullScan_18 | 10000.00    | cop[tikv]    | table:trips       | keep order:false, stats:pseudo                                                                     |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
5 rows in set (0.00 sec)
```

Additional Information in EXPLAIN ANALYZE Output
Different from EXPLAIN, EXPLAIN ANALYZE executes the corresponding SQL statement, records its runtime information, and returns the information together with the execution plan. There runtime information is crucial for debugging query execution.

Description
- actRows: Number of rows output by the operator.
- execution info: Detailed execution information of the operator. time represents the total wall time from entering the operator to leaving the operator, including the total execution time of all sub-operators. If the operator is called many times by the parent operator then the time refers to the accumulated time. loops is the number of times the current operator is called by the parent operator.
- memory: Memory used by the operator.
- disk: Disk space used by the operator.

Note: Some attributes and explain table columns are omitted for improved formatting
```SQL
tidb:db1> EXPLAIN ANALYZE SELECT SUM(pm.m_count)/COUNT(*) FROM
    -> (SELECT COUNT(m.name) m_count
    ->  FROM universe.moons m
    ->  RIGHT JOIN
    ->  (SELECT p.id, p.name
    ->   FROM universe.planet_categories c
    ->   JOIN universe.planets p
    ->   ON c.id = p.category_id AND c.name = 'Jovian') pc
    ->  ON m.planet_id = pc.id
    ->  GROUP BY pc.name) pm;
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| id                                      |.| actRows | task      | access object             | execution info                                                 |.| memory    | disk    |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| Projection_14                           |.| 1       | root      |                           | time:1.39ms, loops:2, RU:1.561975, Concurrency:OFF             |.| 9.64 KB   | N/A     |
| └─StreamAgg_16                          |.| 1       | root      |                           | time:1.39ms, loops:2                                           |.| 1.46 KB   | N/A     |
|   └─Projection_40                       |.| 4       | root      |                           | time:1.38ms, loops:4, Concurrency:OFF                          |.| 8.24 KB   | N/A     |
|     └─HashAgg_17                        |.| 4       | root      |                           | time:1.36ms, loops:4, partial_worker:{...}, final_worker:{...} |.| 82.1 KB   | N/A     |
|       └─HashJoin_19                     |.| 25      | root      |                           | time:1.29ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 2.25 KB   | 0 Bytes |
|         ├─HashJoin_35(Build)            |.| 4       | root      |                           | time:1.08ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 25.7 KB   | 0 Bytes |
|         │ ├─IndexReader_39(Build)       |.| 1       | root      |                           | time:888.5µs, loops:2, cop_task: {...}                         |.| 286 Bytes | N/A     |
|         │ │ └─IndexRangeScan_38         |.| 1       | cop[tikv] | table:c, index:name(name) | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         │ └─TableReader_37(Probe)       |.| 10      | root      |                           | time:543.7µs, loops:2, cop_task: {...}                         |.| 577 Bytes | N/A     |
|         │   └─TableFullScan_36          |.| 10      | cop[tikv] | table:p                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         └─TableReader_22(Probe)         |.| 28      | root      |                           | time:671.7µs, loops:2, cop_task: {...}                         |.| 876 Bytes | N/A     |
|           └─TableFullScan_21            |.| 28      | cop[tikv] | table:m                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
```


### Reading Execution Plans: First Child First

To understand why SQL queries run slowly, it's very important to know how to read Execution Plans. The main rule for reading an execution plan is "first child first – recursive descent".
Each operator of the plan produces rows of data. When we talk about how a plan runs, we really mean the order in which each operator produces its rows. The "first child first" rule means that to produce its rows, each operator of the plan asks its child operators to produce their rows first. Then it combines these rows in some way. The order in which it asks its child parts is the same as the order they appear in the plan.

There are three important details to add to this:

1. Parent-Child Interaction: Although a parent operator calls its child operators in sequence, it may cycle through them multiple times. For example, in an index lookup or nested loop join, the parent fetches a batch of rows from the first child, then (zero or more) rows from the second child, repeating this process until it consumes the entire result set from the first child.

2. Blocking vs. Non-blocking Operators: Operators can be either blocking or non-blocking. Blocking operators, such as `TopN` and `HashAgg`, must create their entire result set before passing anything to their parent. Non-blocking operators, like `IndexLookup` and `IndexJoin`, create and pass their row source piece by piece on demand.

3. Concurrent vs. Serial Execution: Child operators can be executed concurrently or serially. For instance, the child operators of an `IndexLookup` operator are executed serially, while those of a `HashJoin` operator can be executed concurrently.



Let's apply the "first child first – recursive descent" rule to the first plan. When reading an execution plan, you should start from the top and work your down bottom. In the below example begin by looking at the `TableFullScan_18` (or the first child of the tree). In this case the access operator for table trips are implemented using full table scan. The rows produced by the tables scans will be consumed by the `Selection_19` operator. The `Selection_19` operator is to filter the data by `ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000)`. Next the group-by operator `StreamAgg_9` is to implemented the aggregation `count(*)`. Be noted that the 3 operators `TableFullScan_18`, `Selection_19`, `StreamAgg_9` are pushdown to TiKV, which is marked as `cop[tikv]`, so that early filter and aggregation can be done in TiKV, to minize the data transfer between TiKV and TiDB. Finally the `TableReader_21` is to read the data from the `StreamAgg_9` operator, then finally the `StreamAgg_20` is to implemented the aggregation `count(*)`.


```SQL
tidb> EXPLAIN SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';

+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| id                       | estRows     | task         | access object     | operator info                                                                                      |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| StreamAgg_20             | 1.00        | root         |                   | funcs:count(Column#13)->Column#11                                                                  |
| └─TableReader_21         | 1.00        | root         |                   | data:StreamAgg_9                                                                                   |
|   └─StreamAgg_9          | 1.00        | cop[tikv]    |                   | funcs:count(1)->Column#13                                                                          |
|     └─Selection_19       | 250.00      | cop[tikv]    |                   | ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000) |
|       └─TableFullScan_18 | 10000.00    | cop[tikv]    | table:trips       | keep order:false, stats:pseudo                                                                     |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
5 rows in set (0.00 sec)
```
Let's apply the "first child first – recursive descent" rule to the second plan. In the below example begin from the top to bottom, by looking at the `IndexRangeScan_47` (the first child of the tree). For the table `stars`, the optimizer ony need to select the column `name` and `id`, the two columns can be met by the index `name(name)`. So for the table `star`, the root reader is `IndexReader_48`, rather than a `TableReader`.
The join method between `stars` and `planets` is a hash join, which is marked as `HashJoin_44`. The data access method on `planets` is a `TableFullScan_45`. After the join, the `TopN_26` and `TOPN_19` is to implemented the two order by and limit corespondingly. The final operator `Projection_16` is to implemented the column projection for `t5.name`.


```SQL
tidb> EXPLAIN SELECT t5.name FROM
    -> (SELECT p.name, p.gravity, p.distance_from_sun FROM universe.planets p JOIN universe.stars s
    ->  ON s.id = p.sun_id AND s.name = 'Sun'
    ->  ORDER BY p.distance_from_sun ASC LIMIT 5) t5
    -> ORDER BY t5.gravity DESC LIMIT 3;
+-----------------------------------+----------+-----------+---------------------------+
| id                                | estRows  | task      | access object             |
+-----------------------------------+----------+-----------+---------------------------+
| Projection_16                     | 3.00     | root      |                           |
| └─TopN_19                         | 3.00     | root      |                           |
|   └─TopN_26                       | 5.00     | root      |                           |
|     └─HashJoin_44                 | 5.00     | root      |                           |
|       ├─IndexReader_48(Build)     | 1.00     | root      |                           |
|       │ └─IndexRangeScan_47       | 1.00     | cop[tikv] | table:s, index:name(name) |
|       └─TableReader_46(Probe)     | 10.00    | root      |                           |
|         └─TableFullScan_45        | 10.00    | cop[tikv] | table:p                   |
+-----------------------------------+----------+-----------+---------------------------+
```

Here is a figure illustrating the plan tree for the second execution plan:

![execution-plan-traverse](/media/sql-tuning/execution-plan-traverse.png)

The traversal of the execution plan follows a top-to-bottom, first-child-first approach. This traversal pattern corresponds to a postorder (Left, Right, Root) traversal of the plan tree.

To read this plan:

1. Start at the top with Projection_16
2. Move to its child, TopN_19
3. Continue to TopN_26
4. Proceed to HashJoin_44
5. For HashJoin_44, first process its left (Build) child:
   - IndexReader_48
   - IndexRangeScan_47
6. Then process its right (Probe) child:
   - TableReader_46
   - TableFullScan_45

This traversal ensures that each operator's inputs are processed before the operator itself, allowing for efficient execution of the query plan.

### Identifying and Understanding Bottlenecks in Execution Plans

When reading the execution plan, it's crucial to compare the `actRows` (actual rows) with the `estRows` (estimated rows) to assess the accuracy of the optimizer's estimations. A significant discrepancy between these values may indicate that the optimizer's statistics are outdated or inaccurate, potentially leading to suboptimal query plans.

To identify the bottleneck in a poorly performing query:

1. Scan the `execution info` section from top to bottom, looking for operators that consume a significant amount of time.
2. For the first child operator with significant time consumption, analyze the following:
   - `actRows`: Compare with `estRows` to check for estimation accuracy.
   - Detailed measurements in `execution info`: Look for high values in time, loops, or other metrics.
   - `memory` and `disk` usage: High values may indicate suboptimal plan or resource constraints.
3. Correlate these factors to determine the root cause of the performance issue. For example, if you see a `TableFullScan` operation with a high `actRows` count and significant time in `execution info`, it might suggest the need for an index. Alternatively, if a `HashJoin` operation shows high memory usage and time, you might need to optimize the join or consider alternative join methods.

In the execution plan below, the query ran for 5 minutes and 51 seconds before being canceled. Let's analyze the key issues:

1. Severe underestimation: The first child operator `IndexReader_76` reads data from the index `index_orders_on_adjustment_id(adjustment_id)`. The actual number of rows (`actRows`) is 256,811,189, which is drastically higher than the estimated 1 row (`estRows`).

2. Memory overflow: Due to this underestimation, the hash join operator `HashJoin_69` attempts to build a hash table with far more data than anticipated. This results in excessive memory usage (22.6GB) and disk usage (7.65GB).

3. Query termination: The `actRows` is 0 for `HashJoin_69` and subsequent operators, indicating that the hash join consumed too much memory, likely causing the query to be terminated by memory control mechanisms.

4. Incorrect join order: The root cause of this inefficient plan is the severe underestimation of `estRows` for `IndexRangeScan_75`, which led to an incorrect join order decision by the optimizer.

To address these issues, need to ensure that table statistics are up-to-date, especially for the `orders` table and the `index_orders_on_adjustment_id` index.

```SQL
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...----------------------+
| id                                 | estRows   | estCost      | actRows   | task      | access object                                                                          | execution info ...| memory   | disk     |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...----------------------+
| TopN_19                            | 1.01      | 461374372.63 | 0         | root      |                                                                                        | time:5m51.1s, l...| 0 Bytes  | 0 Bytes  |
| └─IndexJoin_32                     | 1.01      | 460915067.45 | 0         | root      |                                                                                        | time:5m51.1s, l...| 0 Bytes  | N/A      |
|   ├─HashJoin_69(Build)             | 1.01      | 460913065.41 | 0         | root      |                                                                                        | time:5m51.1s, l...| 21.6 GB  | 7.65 GB  |
|   │ ├─IndexReader_76(Build)        | 1.00      | 18.80        | 256805045 | root      |                                                                                        | time:1m4.1s, lo...| 12.4 MB  | N/A      |
|   │ │ └─IndexRangeScan_75          | 1.00      | 186.74       | 256811189 | cop[tikv] | table:orders, index:index_orders_on_adjustment_id(adjustment_id)                       | tikv_task:{proc...| N/A      | N/A      |
|   │ └─Projection_74(Probe)         | 30652.93  | 460299612.60 | 1024      | root      |                                                                                        | time:1.08s, loo...| 413.4 KB | N/A      |
|   │   └─IndexLookUp_73             | 30652.93  | 460287375.95 | 6144      | root      | partition:all                                                                          | time:1.08s, loo...| 107.8 MB | N/A      |
|   │     ├─IndexRangeScan_70(Build) | 234759.64 | 53362737.50  | 390699    | cop[tikv] | table:rates, index:index_rates_on_label_id(label_id)                                   | time:29.6ms, lo...| N/A      | N/A      |
|   │     └─Selection_72(Probe)      | 30652.93  | 110373973.91 | 187070    | cop[tikv] |                                                                                        | time:36.8s, loo...| N/A      | N/A      |
|   │       └─TableRowIDScan_71      | 234759.64 | 86944962.10  | 390699    | cop[tikv] | table:rates                                                                            | tikv_task:{proc...| N/A      | N/A      |
|   └─TableReader_28(Probe)          | 0.00      | 43.64        | 0         | root      |                                                                                        |                ...| N/A      | N/A      |
|     └─Selection_27                 | 0.00      | 653.96       | 0         | cop[tikv] |                                                                                        |                ...| N/A      | N/A      |
|       └─TableRangeScan_26          | 1.01      | 454.36       | 0         | cop[tikv] | table:labels                                                                           |                ...| N/A      | N/A      |
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...----------------------+
```

Here is the expected execution plan after fixing the incorrect estimation on the `orders` table. The query now takes 1.96 seconds to run, which is a significant improvement from the previous 5 minutes and 51 seconds:

1. Accurate estimation: The `estRows` values are now much closer to the `actRows`, indicating that the statistics have been updated and are more accurate.
2. Efficient join order: The query now starts with a `TableReader` on the `labels` table, followed by an `IndexJoin` with the `rates` table, and finally another `IndexJoin` with the `orders` table. This join order is more efficient given the actual data distribution.
3. No memory overflow: Unlike the previous plan, there are no signs of excessive memory or disk usage, indicating that the query executes within expected resource limits.
4. Complete execution: All operators show non-zero `actRows`, confirming that the query completes successfully without being terminated due to resource constraints.

This optimized plan demonstrates the importance of accurate statistics and proper join order in query performance. The dramatic reduction in execution time (from 351 seconds to 1.96 seconds) highlights the potential impact of addressing estimation errors and choosing appropriate execution strategies.

```SQL
+---------------------------------------+----------+---------+-----------+----------------------------------------------------------------------------------------+---------------...+----------+------+
| id                                    | estRows  | actRows | task      | access object                                                                          | execution info...| memory   | disk |
+---------------------------------------+----------+---------+-----------+----------------------------------------------------------------------------------------+---------------...+----------+------+
| Limit_24                              | 1000.00  | 1000    | root      |                                                                                        | time:1.96s, lo...| N/A      | N/A  |
| └─IndexJoin_88                        | 1000.00  | 1000    | root      |                                                                                        | time:1.96s, lo...| 1.32 MB  | N/A  |
|   ├─IndexJoin_99(Build)               | 1000.00  | 2458    | root      |                                                                                        | time:1.96s, lo...| 77.7 MB  | N/A  |
|   │ ├─TableReader_109(Build)          | 6505.62  | 158728  | root      |                                                                                        | time:1.26s, lo...| 297.0 MB | N/A  |
|   │ │ └─Selection_108                 | 6505.62  | 171583  | cop[tikv] |                                                                                        | tikv_task:{pro...| N/A      | N/A  |
|   │ │   └─TableRangeScan_107          | 80396.43 | 179616  | cop[tikv] | table:labels                                                                           | tikv_task:{pro...| N/A      | N/A  |
|   │ └─Projection_98(Probe)            | 1000.00  | 2458    | root      |                                                                                        | time:2.13s, lo...| 59.2 KB  | N/A  |
|   │   └─IndexLookUp_97                | 1000.00  | 2458    | root      | partition:all                                                                          | time:2.13s, lo...| 1.20 MB  | N/A  |
|   │     ├─Selection_95(Build)         | 6517.14  | 6481    | cop[tikv] |                                                                                        | time:798.6ms, ...| N/A      | N/A  |
|   │     │ └─IndexRangeScan_93         | 6517.14  | 6481    | cop[tikv] | table:rates, index:index_rates_on_label_id(label_id)                                   | tikv_task:{pro...| N/A      | N/A  |
|   │     └─Selection_96(Probe)         | 1000.00  | 2458    | cop[tikv] |                                                                                        | time:444.4ms, ...| N/A      | N/A  |
|   │       └─TableRowIDScan_94         | 6517.14  | 6481    | cop[tikv] | table:rates                                                                            | tikv_task:{pro...| N/A      | N/A  |
|   └─TableReader_84(Probe)             | 984.56   | 1998    | root      |                                                                                        | time:207.6ms, ...| N/A      | N/A  |
|     └─Selection_83                    | 984.56   | 1998    | cop[tikv] |                                                                                        | tikv_task:{pro...| N/A      | N/A  |
|       └─TableRangeScan_82             | 1000.00  | 2048    | cop[tikv] | table:orders                                                                           | tikv_task:{pro...| N/A      | N/A  |
+---------------------------------------+----------+---------+-----------+----------------------------------------------------------------------------------------+---------------...+----------+------+
```

## Index Strategy in TiDB

TiDB is a distributed SQL database that completely decouples the SQL layer (TiDB) from the storage layer (TiKV). Unlike traditional databases, TiDB does not have a buffer pool to cache data at the compute node. As a result, the performance of SQL queries and the TiDB cluster is closely tied to the number of key-value (KV) requests that need to be processed.

In TiDB, leveraging indexes effectively is crucial for performance tuning, as it can significantly reduce the number of KV RPC (Remote Procedure Call) requests. By minimizing these requests, you can greatly improve query performance and overall system efficiency. Here are some key strategies:

- Avoiding full table scans
- Avoiding sorting
- Skipping row lookups when possible

### Improving Query Speed with a Custom Index

The original query took 11 minutes and 9 seconds to complete, which is an extremely long execution time for a query that only needs to return 101 rows. This poor performance can be attributed to several factors:

1. Inefficient index usage: The optimizer chose the index on created_at, which resulted in scanning 25,147,450 rows.
2. Large intermediate result set: After applying the date range filter, 12,082,311 rows still needed to be processed.
3. Late filtering: The most selective predicates (mode, user_id, and label_id) were applied after accessing the table, resulting in 16,604 rows.
4. Sorting overhead: The final sort operation on 16,604 rows added additional processing time.


Here is the query pattern
```SQL
SELECT `orders`.*
FROM `orders`
WHERE `orders`.`mode` = 'production'
AND `orders`.`user_id` = 11111
AND (orders.label_id IS NOT NULL)
AND (orders.created_at >= '2024-04-07 18:07:52')
AND (orders.created_at <= '2024-05-11 18:07:52')
AND (id >= 1000000000)
AND (id <= 2000000000)
AND (orders.id < 1500000000) 
ORDER BY orders.id DESC LIMIT 101;
```

```SQL
PRIMARY KEY (`id`),
UNIQUE KEY `index_orders_on_adjustment_id` (`adjustment_id`),
KEY `index_orders_on_user_id` (`user_id`),
KEY `index_orders_on_label_id` (`label_id`),
KEY `index_orders_on_created_at` (`created_at`)
```

original plan
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+
| id                             | estRows   | actRows | task      | access object                                                                  | execution info                                      | operator info                                                                                                        | memory   | disk |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+
| TopN_10                        | 101.00    | 101     | root      |                                                                                | time:11m9.8s, loops:2                               | orders.id:desc, offset:0, count:101                                                                                  | 271 KB   | N/A  |
| └─IndexLookUp_39               | 173.83    | 16604   | root      |                                                                                | time:11m9.8s, loops:19, index_task: {total_time:...}|                                                                                                                      | 20.4 MB  | N/A  |
|   ├─Selection_37(Build)        | 8296.70   | 12082311| cop[tikv] |                                                                                | time:26.4ms, loops:11834, cop_task: {num: 294, m...}| ge(orders.id, 1000000000), le(orders.id, 2000000000), lt(orders.id, 1500000000)                                      | N/A      | N/A  |
|   │ └─IndexRangeScan_35        | 6934161.90| 25147450| cop[tikv] | table:orders, index:index_orders_on_created_at(created_at)                     | tikv_task:{proc max:2.15s, min:0s, avg: 58.9ms, ...}| range:[2024-04-07 18:07:52,2024-05-11 18:07:52), keep order:false                                                    | N/A      | N/A  |
|   └─Selection_38(Probe)        | 173.83    | 16604   | cop[tikv] |                                                                                | time:54m46.2s, loops:651, cop_task: {num: 1076, ...}| eq(orders.mode, "production"), eq(orders.user_id, 11111), not(isnull(orders.label_id))                               | N/A      | N/A  |
|     └─TableRowIDScan_36        | 8296.70   | 12082311| cop[tikv] | table:orders                                                                   | tikv_task:{proc max:44.8s, min:0s, avg: 3.33s, p...}| keep order:false                                                                                                     | N/A      | N/A  |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+


Performance Improvement with the New Index:

After creating the new index (new_idx on orders(user_id, mode, id, created_at, label_id)), the query performance improved dramatically. The execution time reduced from 11 minutes and 9 seconds to just 5.3 milliseconds, which is a staggering improvement of over 126,000 times faster. This massive improvement can be explained by:

1. Efficient index usage: The new index allows for an index range scan on user_id, mode, and id, which are the most selective predicates. This drastically reduces the number of rows scanned from millions to just 224.
2. Index-only sort: The 'keep order:true' in the plan indicates that the index structure is used for sorting, avoiding a separate sort operation.
3. Early filtering: The most selective predicates are applied first, quickly reducing the result set to 224 rows before further filtering.
4. Limit push-down: The LIMIT clause is pushed down to the index scan, allowing early termination of the scan once 101 rows are found.

This case demonstrates the profound impact that a well-designed index can have on query performance. By aligning the index structure with the query's predicates, sort order, and required columns, we achieved a performance improvement of over five orders of magnitude.

plan with new index (user_id, mode, id, created_at, label_id) 
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+
| id                             | estRows   | actRows | task      | access object                                                                  | execution info                                      | operator info                                                                                                        | memory   | disk |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+
| IndexLookUp_32                 | 101.00    | 101     | root      |                                                                                | time:5.3ms, loops:2, RU:3.435006, index_task: {t...}| limit embedded(offset:0, count:101)                                                                                  | 128.5 KB | N/A  |
| ├─Limit_31(Build)              | 101.00    | 101     | cop[tikv] |                                                                                | time:1.35ms, loops:1, cop_task: {num: 1, max: 1....}| offset:0, count:101                                                                                                  | N/A      | N/A  |
| │ └─Selection_30               | 535.77    | 224     | cop[tikv] |                                                                                | tikv_task:{time:0s, loops:3}                        | ge(orders.created_at, 2024-04-07 18:07:52), le(orders.created_at, 2024-05-11 18:07:52), not(isnull(orders.label_id)) | N/A      | N/A  |
| │   └─IndexRangeScan_28        | 503893.42 | 224     | cop[tikv] | table:orders, index:index_orders_new(user_id, mode, id, created_at, label_id)  | tikv_task:{time:0s, loops:3}                        | range:[11111 "production" 1000000000,11111 "production" 1500000000), keep order:true, desc                           | N/A      | N/A  |
| └─TableRowIDScan_29(Probe)     | 101.00    | 101     | cop[tikv] | table:orders                                                                   | time:2.9ms, loops:2, cop_task: {num: 3, max: 2.7...}| keep order:false                                                                                                     | N/A      | N/A  |
+--------------------------------+-----------+---------+-----------+--------------------------------------------------------------------------------+-----------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+----------+------+


### general rules for composite index strategy

Make sure to follow the composite index strategy, there is the recommended order for the columns in the index:

1. Equal predicates for index prefix (columns accessed directly):
   - Equal conditions 
   - IS NULL conditions

2. Columns after index prefix for sorting:
   - Leverage index to preprocess sorting
   - Ensure sort and limit pushdown to TiKV
   - Maintain sorted order

3. Additional filtering columns:
   - Non-equal predicates (!=, <>, IS NOT NULL, etc)
   - Time range conditions on datetime columns
   - Helps reduce row lookups

4. columns in index postfix for select list or aggregate function:
   - Leverage IndexReader
   - Avoid IndexLookup operations


### The Cost of Indexing

While indexes can significantly improve query performance, they also come with costs that should be carefully considered:

1. Performance Impact on Writes:
   - Each additional index slows down write operations (INSERT, UPDATE, DELETE)
   - When data is modified, all affected indexes must be updated
   - This overhead increases with each additional index

2. Resource Consumption:
   - Indexes require additional disk space
   - More memory is needed to cache frequently accessed indexes
   - Backup and recovery operations take longer

3. Write Hotspot Risk:
   - Secondary indexes can create write hotspots, for example, a datetime monotonically increasing index will cause the hotspots on the write of the table
   - Performance can degrade significantly if hotspots occur

Best Practice:
- Only create indexes that provide clear performance benefits
- Regularly review index usage statistics via [TIDB_INDEX_USAGE](https://docs.pingcap.com/tidb/dev/information-schema-tidb-index-usage)
- Consider the write/read ratio of your workload when designing indexes