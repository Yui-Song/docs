---
title: SQL Tuning Best Practice
summary: Learn how to do SQL tuning in TiDB

---

# Introduction to SQL Tuning

SQL tuning is a critical aspect of optimizing database system performance. It involves a systematic approach to improve the efficiency of SQL queries. The process typically consists of three key steps:

1. Identify high-impact SQL statements:
   - Review SQL execution history to find statements that consume a large portion of system resources or contribute significantly to the application workload.
   - Use monitoring tools and performance metrics to pinpoint these resource-intensive queries.

2. Analyze execution plans:
   - Examine the execution plans generated by the query optimizer for the identified statements.
   - Verify that these plans are reasonably efficient and utilize appropriate indexes and join methods.

3. Implement optimizations:
   - Apply corrective actions to improve poorly performing SQL statements.
   - This may include rewriting queries, adding or modifying indexes, updating statistics, or adjusting database parameters.

These steps are iteratively repeated until:
- The system performance meets the desired targets, or
- No further improvements can be made to the remaining statements.

It's important to note that SQL tuning is an ongoing process. As your data volumes grow and query patterns evolve, you should:
- Regularly monitor query performance
- Re-evaluate your optimization strategies
- Adapt your approach to address new performance challenges

By consistently applying these practices, you can ensure that your database maintains optimal performance over time.

# Goals for Tuning

The primary objectives of SQL tuning are to:

1. Reduce response time for end users
2. Minimize resource consumption for processing workloads

These goals can be achieved through various strategies:

## Optimize Query Execution

SQL tuning often involves finding more efficient ways to process the same workload without changing the query's functionality. This can be done by:

1. Improving execution plans:
   - Analyze and modify query structures to enable more efficient processing
   - Utilize appropriate indexes to reduce data access and processing time
   - Enable TiFlash for analytical queries on large volumes of data and Leverage MPP (Massively Parallel Processing) engine for complex aggregations and joins

2. Enhancing data access methods:
   - Use covering indexes to satisfy queries directly from the index, avoiding table access
   - Implement partitioning strategies to limit data scans to relevant partitions

Examples:

- Creating an index for frequently queried columns can significantly reduce resource usage, especially for queries that access a small percentage of table data.
- Utilizing index-only scans for queries that return a limited number of sorted results can avoid full table scans and sorting operations.

## Balance Workload Distribution

In TiDB's distributed architecture, ensuring even workload distribution across multiple TiKV nodes is crucial for optimal performance:

- Prevent hotspots: Design schemas and queries to avoid concentrated read or write operations on specific nodes
- Utilize parallel processing:
   - Design queries to take advantage of TiDB's distributed execution capabilities
   - Enable and tune parallel execution settings for resource-intensive operations

By implementing these strategies, you can ensure that your TiDB cluster efficiently utilizes all available resources and avoids bottlenecks caused by uneven workload distribution or serialization on individual TiKV nodes.

# Identifying High-Load SQL
The most efficient way to identify Resource-Intensive SQL is using TiDB Dashboard, There are other tools like views and logs available as well.

## Monitoring SQL Statements by Using TiDB Dashboard
### SQL Statements Panel 
In TiDB Dashboard, navigate to SQL Statements panel, which helps us identify the following:

1. The SQL statement with the highest total latency is the one that takes the longest time to execute out of multiple executions of the same SQL statement. 
2. It can also display the number of times each SQL statement has been executed cumulatively, allowing us to find the SQL statement with the highest execution frequency.
3. Clicking on each SQL statement allows us to delve deeper into the EXPLAIN ANALYZE results.

SQL statements are normalized as templates, where literals and bind variables are replaced by '?'. This normalization and sorting process allows you to quickly pinpoint the most resource-intensive queries that may require optimization.
![sql-statements-default](/media/sql-tuning/sql-statements-default.png)


### Slow Queries Panel Default Display
In the TiDB Dashboard, we can find the Slow Query panel, which displays all SQL statements whose execution time exceeds the time threshold set by the system variable tidb_slow_log_threshold (default 300 milliseconds). 

On Slow Queries panel, we can find:

1. The slowest SQL queries.
2. The SQL query that reads the most data from TiKV.
3. The EXPLAIN ANALYZE output from drilling down the SQL statement by clicking it.
4. Please note that on the Slow Queries panel, we cannot get the frequency of the SQL statement execution. Once the execution elapsed time exceeds tidb_slow_log_threshold for single instance, the query is then listed on the Slow Queries panel.
![slow-query-default](/media/sql-tuning/slow-query-default.png)

## Other Tools for Identifying Top SQL

In addition to TiDB Dashboard, there are several other tools available to identify resource-intensive SQL queries:

Each tool offers unique insights and can be valuable for different analysis scenarios. Using a combination of these tools allows for comprehensive SQL performance monitoring and optimization.
- [slow query log](https://docs.pingcap.com/tidb/stable/identify-slow-queries)
- [statements_summary view](https://docs.pingcap.com/tidb/stable/statement-summary-tables#statements_summary)
- [top sql feature](https://docs.pingcap.com/tidb/stable/top-sql)
- [expensive queries in tidb log](https://docs.pingcap.com/tidb/stable/identify-expensive-queries)
- [cluster_processlist view](https://docs.pingcap.com/tidb/stable/information-schema-processlist#cluster_processlist)


## Gathering Data on the SQL Identified

For the top SQL statements identified, you can use [PLAN REPLAYER](https://docs.pingcap.com/tidb/stable/sql-plan-replayer) to capture and save the on-site information of a TiDB cluster. This tool allows you to recreate the execution environment for further analysis. The syntax for exporting SQL information is as follows:

```SQL
PLAN REPLAYER DUMP EXPLAIN [ANALYZE] [WITH STATS AS OF TIMESTAMP expression] sql-statement;
```

Use EXPLAIN ANALYZE whenever possible, as it captures actual execution information in addition to the execution plan. This provides more accurate insights into query performance.

# SQL Tuning Guide

This guide focuses on providing actionable advice for beginners looking to optimize their SQL queries in TiDB. By following these best practices, you can ensure better query performance and SQL Tuning. We'll cover below topic:
- Query Processing Workflow
- Optimizer Fundamentals
- Statistics Management
- How TiDB build A Execution Plan
- Understand Execution Plan
- Real-World Use Case
  - Bad Plan Debug
  - Index Strategy in TiDB
  - Partition table: local index vs global index
  - Index Full Scan is fater than Table Full Scan

## Query Processing Workflow
the client sends a SQL statement to the protocol layer of TiDB server. The protocol layer is responsible for handling the connection between TiDB Server and the client, receiving SQL statement from the client, and returning data to the client.

To the right of the protocol layer is the Optimizer layer of TiDB Server, which is responsible for processing SQL statements. The process is as follows:

1. SQL statement arrives at the SQL optimizer through the protocol layer and is first parsed into an Abstract Syntax Tree (AST).
2. Pre-Process is primarily for Point Get. If it is a Point Get, the following-up optimization processes can be skipped, the next step jumps to the SQL Executor.
3. After confirming that it is not a Point Get, the AST goes to the Logical Transformation, which rewrites the SQL logically based on certain rules.
4. The AST that has gone through Logical Transformation will undergo Cost-Based Optimization.
5. During Cost-Based Optimization, the optimizer considers statistics to determine how to select specific operators and finally generates a physically executable physical execution plan.
6. The generated physical execution plan is sent to the SQL Executor of the TiDB database for execution.
7. Unlike traditional databases, TiDB databases need to push down the execution plan to different TiKV for reading and writing data.

![workflow](/media/sql-tuning/workflow.png)

## Optimizer Fundamentals

TiDB uses a cost-based optimizer (CBO) to determine the most efficient execution plan for a SQL statement. This optimizer evaluates different execution strategies and chooses the one with the lowest estimated cost. The cost is influenced by factors such as:

- SQL
- Schema Design
- Statistics
  - Table
  - Index
  - Column

Based on the input, The cost model will produce the execution plan, which includes the details how the system execute the sql, including 
- Access Method
- Join Method
- Join Order

The optimizer is as good as the information it receives. Therefore, ensuring up-to-date statistics and well-designed indexes is critical.

## Statistics Management

Statistics are essential to the TiDB optimizer. TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement.

statistics is generally divided into two levels: table level and column level. 
- For table-level statistics, it includes the total number of rows in the table and the number of rows that have been modified since the last collection of statistics. 
- The column-level statistics information is more abundant, including histograms, Count-Min Sketch, Top-N (values or indexes with the highest occurrences), distribution and quantity of different values, and the number of null values, and so on.

To ensure the statistics are healthy and representative, you can use the following commands:

1. `SHOW STATS_META`: This command provides metadata about table statistics.
2. `SHOW STATS_HEALTHY`: This command shows the health status of table statistics.

For example, you can use:

```SQL
tidb> SHOW STATS_META WHERE table_name='T2'\G;
*************************** 1. row ***************************
 Db_name: test
 Table_name: T2
Partition_name:
 Update_time: 2023-05-11 02:16:50
 Modify_count: 20000
 Row_count: 20000
1 row in set (0.03 sec)

tidb> SHOW STATS_HEALTHY WHERE table_name='T2'\G;
*************************** 1. row ***************************
    Db_name: test
  Table_name: T2
Partition_name:
    Healthy: 0
1 row in set (0.00 sec)

```

In TiDB database, there are two ways to collect statistics: automatic collection and manual collection. In most case, the auto collection job works fine. Automatic collection is actually triggered when certain conditions are met for a table, and TiDB will automatically collect statistics. We commonly use three triggering conditions, which are: ratio, start_time and end_time.
tidb_auto_analyze_ratio: The healthiness trigger
tidb_auto_analyze_start_time and tidb_auto_analyze_end_time: The allowed job window
```SQL
mysql> show variables like '%auto_analyze%';
+-----------------------------------------+-------------+
| Variable_name                           | Value       |
+-----------------------------------------+-------------+
| tidb_auto_analyze_ratio                 | 0.5         |
| tidb_auto_analyze_start_time            | 00:00 +0000 |
| tidb_auto_analyze_end_time              | 23:59 +0000 |
+-----------------------------------------+-------------+
```

In cases where automatic collection doesn't meet your needs, you can manually collect statistics using the `ANALYZE TABLE table_name` statement. This allows you to:

1. Adjust the sample rate for more accurate or faster analysis
2. Increase the number of top-N values collected
3. Gather statistics for specific columns only

It's important to note that after manual collection, subsequent automatic gathering jobs will inherit the new parameters. This means that any customizations you've made during manual collection will be carried forward in future automatic analyses.

Another common scenario is locking table statistics. This is useful when:
1. The statistics on the table are already representative of the data.
2. The table is very large and statistics collection is time-consuming.
3. You want to maintain statistics only during specific time windows.

To lock the statistics for a table, you can use the following command `LOCK STATS table_name`.

for more detail about statistics, please refer to [statistics](https://docs.pingcap.com/tidb/stable/statistics).

## How TiDB build A Execution Plan
An SQL statement undergoes optimization primarily in the optimizer through three stages:
- Pre-Processing
- Logical Transformation
- Cost-based Optimization

### Pre-Processing
The main actions in the pre-processing stage it to determine if the SQL statement can be executed by using Point_Get or Batch_Point_Get.

Point_Get or Batch_Point_Get is to get 1 or 0 or many row only by using the TiKV key, the explicit or implicit (_tidb_rowid) primary key. For example, when id column is the primary key of a clustered index table, Point_Get is used to get the particular row. If a plan is identified as Point_Get, optimizer will skip the logical transformation and cost-based optimization.

```SQL
SELECT id, name FROM emp WHERE id = 901; 
```

### Logical Transformation

The purpose of logical Transformation is to optimize the execution of statements based on the characteristics of SELECT list, WHERE predicates, and other predicates in SQL queries. It generates a logical execution plan to annotate and rewrite the query. This logical plan is then passed to the Cost-Based Optimization. The optimization rules include column pruning，partition pruning，eliminate Max/Min, eliminate outer join, join reorder, predicates push-down，subquery rewrite，derive TopN from window functions，and de-correlation of correlated Subquery. Since this step is fully automated by the query optimizer, it usually does not require manual adjustments.

More Detail for Logical Transformation: https://docs.pingcap.com/tidb/stable/sql-logical-optimization.

### Cost-Based Optimization

TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement, and associates a cost with each plan step. The Cost-Based Optimization estimates the cost of each available plan choice, including index accesses and the sequence of table joins, and produces a cost for each available plan. The optimizer then picks the execution plan with the lowest overall cost.

The below figure illustrates the various data access paths and row set operations that cost-based optimization can consider to develop the optimal execution plan. Furthermore, during the logical transformation stage, the query has already been rewritten for predicate push-down. In the cost-based optimization stage, TiKV expression push-down is further implemented when possible at TiKV layer. 

Furthermore, confirming the algorithm for certain SQL operations, such as aggregation, join, and sorting, is essential. For instance, the aggregation operator may utilize either HASH_AGG or STREAM_AGG, while the join operator can select from HASH JOIN, MERGE JOIN, or INDEX JOIN. Likewise, various options are available for the sorting operator.

![cost-based-optimization](/media/sql-tuning/cost-based-optimization.png)

## Understanding Execution Plans

The execution plan represents the steps TiDB will follow to execute a SQL query. In this section, we will learn how to display and read the execution plan.

### Generating and Displaying Execution Plans

Beside access the execution plan information through TiDB Dashboard, TiDB provides a `EXPLAIN` statement to display the execution plan for a SQL query. Here's an example of using `EXPLAIN`:
- id: Operator name and the step unique identifier
- estRows: Estimated number of rows from the particular step
- task: The executor of the step
- access object: The object where the row sources are located
- operator info: Extended information about the operator regarding the step

```SQL
tidb> EXPLAIN SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';

+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| id                       | estRows     | task         | access object     | operator info                                                                                      |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| StreamAgg_20             | 1.00        | root         |                   | funcs:count(Column#13)->Column#11                                                                  |
| └─TableReader_21         | 1.00        | root         |                   | data:StreamAgg_9                                                                                   |
|   └─StreamAgg_9          | 1.00        | cop[tikv]    |                   | funcs:count(1)->Column#13                                                                          |
|     └─Selection_19       | 250.00      | cop[tikv]    |                   | ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000) |
|       └─TableFullScan_18 | 10000.00    | cop[tikv]    | table:trips       | keep order:false, stats:pseudo                                                                     |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
5 rows in set (0.00 sec)
```

Additional Information in EXPLAIN ANALYZE Output
Different from EXPLAIN, EXPLAIN ANALYZE executes the corresponding SQL statement, records its runtime information, and returns the information together with the execution plan. There runtime information is crucial for debugging query execution.

Description
- actRows: Number of rows output by the operator.
- execution info: Detailed execution information of the operator. time represents the total wall time from entering the operator to leaving the operator, including the total execution time of all sub-operators. If the operator is called many times by the parent operator then the time refers to the accumulated time. loops is the number of times the current operator is called by the parent operator.
- memory: Memory used by the operator.
- disk: Disk space used by the operator.

Note: Some attributes and explain table columns are omitted for improved formatting
```SQL
tidb:db1> EXPLAIN ANALYZE SELECT SUM(pm.m_count)/COUNT(*) FROM
    -> (SELECT COUNT(m.name) m_count
    ->  FROM universe.moons m
    ->  RIGHT JOIN
    ->  (SELECT p.id, p.name
    ->   FROM universe.planet_categories c
    ->   JOIN universe.planets p
    ->   ON c.id = p.category_id AND c.name = 'Jovian') pc
    ->  ON m.planet_id = pc.id
    ->  GROUP BY pc.name) pm;
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| id                                      |.| actRows | task      | access object             | execution info                                                 |.| memory    | disk    |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| Projection_14                           |.| 1       | root      |                           | time:1.39ms, loops:2, RU:1.561975, Concurrency:OFF             |.| 9.64 KB   | N/A     |
| └─StreamAgg_16                          |.| 1       | root      |                           | time:1.39ms, loops:2                                           |.| 1.46 KB   | N/A     |
|   └─Projection_40                       |.| 4       | root      |                           | time:1.38ms, loops:4, Concurrency:OFF                          |.| 8.24 KB   | N/A     |
|     └─HashAgg_17                        |.| 4       | root      |                           | time:1.36ms, loops:4, partial_worker:{...}, final_worker:{...} |.| 82.1 KB   | N/A     |
|       └─HashJoin_19                     |.| 25      | root      |                           | time:1.29ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 2.25 KB   | 0 Bytes |
|         ├─HashJoin_35(Build)            |.| 4       | root      |                           | time:1.08ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 25.7 KB   | 0 Bytes |
|         │ ├─IndexReader_39(Build)       |.| 1       | root      |                           | time:888.5µs, loops:2, cop_task: {...}                         |.| 286 Bytes | N/A     |
|         │ │ └─IndexRangeScan_38         |.| 1       | cop[tikv] | table:c, index:name(name) | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         │ └─TableReader_37(Probe)       |.| 10      | root      |                           | time:543.7µs, loops:2, cop_task: {...}                         |.| 577 Bytes | N/A     |
|         │   └─TableFullScan_36          |.| 10      | cop[tikv] | table:p                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         └─TableReader_22(Probe)         |.| 28      | root      |                           | time:671.7µs, loops:2, cop_task: {...}                         |.| 876 Bytes | N/A     |
|           └─TableFullScan_21            |.| 28      | cop[tikv] | table:m                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
```


### Reading Execution Plans: First Child First

To understand why SQL queries run slowly, it's very important to know how to read Execution Plans. The main rule for reading an execution plan is "first child first – go down step by step".
Each operator of the plan produces rows of data. When we talk about how a plan runs, we really mean the order in which each operator produces its rows. The "first child first" rule means that to produce its rows, each operator of the plan asks its child operators to produce their rows first. Then it combines these rows in some way. The order in which it asks its child parts is the same as the order they appear in the plan.

There are two important details to add to this.

First, although a parent operator will call its child operators in turn, the parent may cycle through the child operators multiple times. The obvious example of this is the index lookup or nested loop join, where the parent will fetch a row from its first child then fetch (zero or more) rows from its second child, then fetch the next row from its first child then call its second child again, and repeat until is has consumed the entire resultset from the first child.
Secondly, an operation may be blocking or non-blocking, and it’s worth remembering this when thinking about whether you want to tune a query for latency (time to first row(s) returned) or throughput (time to last row returned). Some operations will have to create their entire resultset before they pass anything up to their parent (blocking); some operations will create and pass their rowsource piece by piece on demand.



Let's apply the "first child first – recursive descent" rule to the first plan. When reading an execution plan, you should start from the top and work your down bottom. In the below example begin by looking at the `TableFullScan_18` (or the first child of the tree). In this case the access operator for table trips are implemented using full table scan. The rows produced by the tables scans will be consumed by the `Selection_19` operator. The `Selection_19` operator is to filter the data by `ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000)`. Next the group-by operator `StreamAgg_9` is to implemented the aggregation `count(*)`. Be noted that the 3 operators `TableFullScan_18`, `Selection_19`, `StreamAgg_9` are pushdown to TiKV, which is marked as `cop[tikv]`, so that early filter and aggregation can be done in TiKV, to minize the data transfer between TiKV and TiDB. Finally the `TableReader_21` is to read the data from the `StreamAgg_9` operator, then finally the `StreamAgg_20` is to implemented the aggregation `count(*)`.


```SQL
tidb> EXPLAIN SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';

+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| id                       | estRows     | task         | access object     | operator info                                                                                      |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
| StreamAgg_20             | 1.00        | root         |                   | funcs:count(Column#13)->Column#11                                                                  |
| └─TableReader_21         | 1.00        | root         |                   | data:StreamAgg_9                                                                                   |
|   └─StreamAgg_9          | 1.00        | cop[tikv]    |                   | funcs:count(1)->Column#13                                                                          |
|     └─Selection_19       | 250.00      | cop[tikv]    |                   | ge(trips.start_date, 2017-07-01 00:00:00.000000), le(trips.start_date, 2017-07-01 23:59:59.000000) |
|       └─TableFullScan_18 | 10000.00    | cop[tikv]    | table:trips       | keep order:false, stats:pseudo                                                                     |
+--------------------------+-------------+--------------+-------------------+----------------------------------------------------------------------------------------------------+
5 rows in set (0.00 sec)
```

Let's apply the "first child first – recursive descent" rule to the second plan. In the below example begin by looking at the `IndexRangeScan_38` (the first child of the tree). For the table `planet_categories`, the optimizer ony need to select the column `name` and `id`, the two columns can be met by the index `name(name)`. So for the table `planet_categories`, the root reader is `IndexReader_39`, rather than a `TableReader`.
The join method between `planet_categories` and `planets` is a hash join, which is marked as `HashJoin_35`. The data access method on `planets` is a `TableFullScan_36`. The join method between `HashJoin_35` and table `moons` is a `HashJoin_19`. The data access method on `moons` is a `TableFullScan_21`. After the join, the `HashAgg_17` is to implemented the aggregation `SUM(pm.m_count)/COUNT(*)`.

When reading the execution plan, it's crucial to compare the `actRows` (actual rows) with the `estRows` (estimated rows) to assess the accuracy of the optimizer's estimations. A significant discrepancy between these values may indicate that the optimizer's statistics are outdated or inaccurate, potentially leading to suboptimal query plans.

To identify the bottleneck in a poorly performing query:

1. Scan the `execution info` section from top to bottom, looking for operators that consume a significant amount of time.
2. For the first operator with significant time consumption, analyze the following:
   - `actRows`: Compare with `estRows` to check for estimation accuracy.
   - Detailed measurements in `execution info`: Look for high values in time, loops, or other metrics.
   - `memory` and `disk` usage: High values may indicate suboptimal plan or resource constraints.
3. Correlate these factors to determine the root cause of the performance issue. For example, if you see a `TableFullScan` operation with a high `actRows` count and significant time in `execution info`, it might suggest the need for an index. Alternatively, if a `HashJoin` operation shows high memory usage and time, you might need to optimize the join or consider alternative join methods.

```SQL
tidb:db1> EXPLAIN ANALYZE SELECT SUM(pm.m_count)/COUNT(*) FROM
    -> (SELECT COUNT(m.name) m_count
    ->  FROM universe.moons m
    ->  RIGHT JOIN
    ->  (SELECT p.id, p.name
    ->   FROM universe.planet_categories c
    ->   JOIN universe.planets p
    ->   ON c.id = p.category_id AND c.name = 'Jovian') pc
    ->  ON m.planet_id = pc.id
    ->  GROUP BY pc.name) pm;
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| id                                      |.| actRows | task      | access object             | execution info                                                 |.| memory    | disk    |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
| Projection_14                           |.| 1       | root      |                           | time:1.39ms, loops:2, RU:1.561975, Concurrency:OFF             |.| 9.64 KB   | N/A     |
| └─StreamAgg_16                          |.| 1       | root      |                           | time:1.39ms, loops:2                                           |.| 1.46 KB   | N/A     |
|   └─Projection_40                       |.| 4       | root      |                           | time:1.38ms, loops:4, Concurrency:OFF                          |.| 8.24 KB   | N/A     |
|     └─HashAgg_17                        |.| 4       | root      |                           | time:1.36ms, loops:4, partial_worker:{...}, final_worker:{...} |.| 82.1 KB   | N/A     |
|       └─HashJoin_19                     |.| 25      | root      |                           | time:1.29ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 2.25 KB   | 0 Bytes |
|         ├─HashJoin_35(Build)            |.| 4       | root      |                           | time:1.08ms, loops:2, build_hash_table:{...}, probe:{...}      |.| 25.7 KB   | 0 Bytes |
|         │ ├─IndexReader_39(Build)       |.| 1       | root      |                           | time:888.5µs, loops:2, cop_task: {...}                         |.| 286 Bytes | N/A     |
|         │ │ └─IndexRangeScan_38         |.| 1       | cop[tikv] | table:c, index:name(name) | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         │ └─TableReader_37(Probe)       |.| 10      | root      |                           | time:543.7µs, loops:2, cop_task: {...}                         |.| 577 Bytes | N/A     |
|         │   └─TableFullScan_36          |.| 10      | cop[tikv] | table:p                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
|         └─TableReader_22(Probe)         |.| 28      | root      |                           | time:671.7µs, loops:2, cop_task: {...}                         |.| 876 Bytes | N/A     |
|           └─TableFullScan_21            |.| 28      | cop[tikv] | table:m                   | tikv_task:{time:0s, loops:1}, scan_detail: {...}               |.| N/A       | N/A     |
+-----------------------------------------+.+---------+-----------+---------------------------+----------------------------------------------------------------+.+-----------+---------+
```

# 4. Real-World Use Cases

## Debugging a Bad Plan
When a SQL query performs poorly, it’s usually due to an inefficient execution plan. Key steps to debug:

Compare estimated rows and actual rows from the execution plan. Large discrepancies indicate a problem with the statistics or index selection.
Ensure appropriate indexes are being used. If the query scans large portions of data, it may require better indexing or statistics updates.

## Index Strategy in TiDB
Indexes play a critical role in query performance. A well-designed index can drastically reduce query execution time.

Make sure to follow the composite index strategy
- Key predicates for index prefix columns, predicates on columns that data can be accessed directly: 
- Columns After Index Prefix, leverage index to preprocess the sorting
- Additional Columns for Filtering, to reduce row lookups

## Partition Tables: Local vs Global Indexes
Partitioning large tables can improve query performance, but the choice between local and global indexes is crucial.

- Local Index: Indexed data is limited to a partition, leading to efficient access within that partition.
- Global Index: Spanning across partitions, global indexes help avoid partition lookups.
Best Practice: Use global indexes for queries accessing multiple partitions to reduce lookup overhead.

## Index Full Scan vs Table Full Scan
In most cases, Index Full Scans are much faster than Table Full Scans because indexes are smaller and ordered. Use index full scans to minimize data access time.

Best Practice:

Ensure that queries make use of index scans whenever possible. A full table scan should only happen when absolutely necessary.


# Key Takeaways
- The quality of the execution plan is dependent on the quality of the inputs. Ensure statistics are up to date and reflect the data accurately.
- Compare estimated rows and actual rows to verify that the optimizer has accurate statistics.
- Follow the composite index strategy:
  - Use key predicates for index prefix columns.
  - Leverage columns after the index prefix to support sorting and filtering.
 . - Add extra columns to reduce row lookups.
- Choose between global and local indexes for partitioned tables based on the query access pattern. Use global indexes to avoid lookup overhead across partitions.

