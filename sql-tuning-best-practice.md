---
title: SQL Tuning Best Practice
summary: Learn how to do SQL tuning in TiDB

---

# Introduction to SQL Tuning

SQL tuning is a critical aspect of optimizing database system performance. It involves a systematic approach to improve the efficiency of SQL queries. The process typically consists of three key steps:

1. Identify high-impact SQL statements:
   - Review SQL execution history to find statements that consume a large portion of system resources or contribute significantly to the application workload.
   - Use monitoring tools and performance metrics to pinpoint these resource-intensive queries.

2. Analyze execution plans:
   - Examine the execution plans generated by the query optimizer for the identified statements.
   - Verify that these plans are reasonably efficient and utilize appropriate indexes and join methods.

3. Implement optimizations:
   - Apply corrective actions to improve poorly performing SQL statements.
   - This may include rewriting queries, adding or modifying indexes, updating statistics, or adjusting database parameters.

These steps are iteratively repeated until:
- The system performance meets the desired targets, or
- No further improvements can be made to the remaining statements.

It's important to note that SQL tuning is an ongoing process. As your data volumes grow and query patterns evolve, you should:
- Regularly monitor query performance
- Re-evaluate your optimization strategies
- Adapt your approach to address new performance challenges

By consistently applying these practices, you can ensure that your database maintains optimal performance over time.

# Goals for Tuning

The primary objectives of SQL tuning are to:

1. Reduce response time for end users
2. Minimize resource consumption for processing workloads

These goals can be achieved through various strategies:

## Optimize Query Execution

SQL tuning often involves finding more efficient ways to process the same workload without changing the query's functionality. This can be done by:

1. Improving execution plans:
   - Analyze and modify query structures to enable more efficient processing
   - Utilize appropriate indexes to reduce data access and processing time
   - Enable TiFlash for analytical queries on large volumes of data and Leverage MPP (Massively Parallel Processing) engine for complex aggregations and joins

2. Enhancing data access methods:
   - Use covering indexes to satisfy queries directly from the index, avoiding table access
   - Implement partitioning strategies to limit data scans to relevant partitions

Examples:

- Creating an index for frequently queried columns can significantly reduce resource usage, especially for queries that access a small percentage of table data.
- Utilizing index-only scans for queries that return a limited number of sorted results can avoid full table scans and sorting operations.

## Balance Workload Distribution

In TiDB's distributed architecture, ensuring even workload distribution across multiple TiKV nodes is crucial for optimal performance:

- Prevent hotspots: Design schemas and queries to avoid concentrated read or write operations on specific nodes
- Utilize parallel processing:
   - Design queries to take advantage of TiDB's distributed execution capabilities
   - Enable and tune parallel execution settings for resource-intensive operations

By implementing these strategies, you can ensure that your TiDB cluster efficiently utilizes all available resources and avoids bottlenecks caused by uneven workload distribution or serialization on individual TiKV nodes.

# Identifying High-Load SQL
The most efficient way to identify Resource-Intensive SQL is using TiDB Dashboard, There are other tools like views and logs available as well.

## Monitoring SQL Statements by Using TiDB Dashboard
### SQL Statements Panel 
In TiDB Dashboard, navigate to SQL Statements panel, which helps us identify the following:

1. The SQL statement with the highest total latency is the one that takes the longest time to execute out of multiple executions of the same SQL statement. 
2. It can also display the number of times each SQL statement has been executed cumulatively, allowing us to find the SQL statement with the highest execution frequency.
3. Clicking on each SQL statement allows us to delve deeper into the EXPLAIN ANALYZE results.

SQL statements are normalized as templates, where literals and bind variables are replaced by '?'. This normalization and sorting process allows you to quickly pinpoint the most resource-intensive queries that may require optimization.
![sql-statements-default](/media/sql-tuning/sql-statements-default.png)


### Slow Queries Panel Default Display
In the TiDB Dashboard, we can find the Slow Query panel, which displays all SQL statements whose execution time exceeds the time threshold set by the system variable tidb_slow_log_threshold (default 300 milliseconds). 

On Slow Queries panel, we can find:

1. The slowest SQL queries.
2. The SQL query that reads the most data from TiKV.
3. The EXPLAIN ANALYZE output from drilling down the SQL statement by clicking it.
4. Please note that on the Slow Queries panel, we cannot get the frequency of the SQL statement execution. Once the execution elapsed time exceeds tidb_slow_log_threshold for single instance, the query is then listed on the Slow Queries panel.
![slow-query-default](/media/sql-tuning/slow-query-default.png)

## Other Tools for Identifying Top SQL

In addition to TiDB Dashboard, there are several other tools available to identify resource-intensive SQL queries:

Each tool offers unique insights and can be valuable for different analysis scenarios. Using a combination of these tools allows for comprehensive SQL performance monitoring and optimization.
- [slow query log](https://docs.pingcap.com/tidb/stable/identify-slow-queries)
- [statements_summary view](https://docs.pingcap.com/tidb/stable/statement-summary-tables#statements_summary)
- [top sql feature](https://docs.pingcap.com/tidb/stable/top-sql)
- [expensive queries in tidb log](https://docs.pingcap.com/tidb/stable/identify-expensive-queries)
- [cluster_processlist view](https://docs.pingcap.com/tidb/stable/information-schema-processlist#cluster_processlist)


## Gathering Data on the SQL Identified

For the top SQL statements identified, you can use [PLAN REPLAYER](https://docs.pingcap.com/tidb/stable/sql-plan-replayer) to capture and save the on-site information of a TiDB cluster. This tool allows you to recreate the execution environment for further analysis. The syntax for exporting SQL information is as follows:

```SQL
PLAN REPLAYER DUMP EXPLAIN [ANALYZE] [WITH STATS AS OF TIMESTAMP expression] sql-statement;
```

Use EXPLAIN ANALYZE whenever possible, as it captures actual execution information in addition to the execution plan. This provides more accurate insights into query performance.

# SQL Tuning Guide

This guide focuses on providing actionable advice for beginners looking to optimize their SQL queries in TiDB. By following these best practices, you can ensure better query performance and SQL Tuning. We'll cover below topic:
- Query Processing Workflow
- Optimizer Fundamentals
- Statistics Management
- Understand Execution Plan
- Real-World Use Case
  - Bad Plan Debug
  - Index Strategy in TiDB
  - Partition table: local index vs global index
  - Index Full Scan is fater than Table Full Scan

## Query Processing Workflow
the client sends a SQL statement to the protocol layer of TiDB server. The protocol layer is responsible for handling the connection between TiDB Server and the client, receiving SQL statement from the client, and returning data to the client.

To the right of the protocol layer is the Optimizer layer of TiDB Server, which is responsible for processing SQL statements. The process is as follows:

1. SQL statement arrives at the SQL optimizer through the protocol layer and is first parsed into an Abstract Syntax Tree (AST).
2. Pre-Process is primarily for Point Get. If it is a Point Get, the following-up optimization processes can be skipped, the next step jumps to the SQL Executor.
3. After confirming that it is not a Point Get, the AST goes to the Logical Transformation, which rewrites the SQL logically based on certain rules.
4. The AST that has gone through Logical Transformation will undergo Cost-Based Optimization.
5. During Cost-Based Optimization, the optimizer considers statistics to determine how to select specific operators and finally generates a physically executable physical execution plan.
6. The generated physical execution plan is sent to the SQL Executor of the TiDB database for execution.
7. Unlike traditional databases, TiDB databases need to push down the execution plan to different TiKV for reading and writing data.

![workflow](/media/sql-tuning/workflow.png)

## Optimizer Fundamentals

TiDB uses a cost-based optimizer (CBO) to determine the most efficient execution plan for a SQL statement. This optimizer evaluates different execution strategies and chooses the one with the lowest estimated cost. The cost is influenced by factors such as:

- SQL
- Schema Design
- Statistics
  - Table
  - Index
  - Column

Based on the input, The cost model will produce the execution plan, which includes the details how the system execute the sql, including 
- Access Method
- Join Method
- Join Order

The optimizer is as good as the information it receives. Therefore, ensuring up-to-date statistics and well-designed indexes is critical.

## Statistics Management

Statistics are essential to the TiDB optimizer. TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement.

statistics is generally divided into two levels: table level and column level. 
- For table-level statistics, it includes the total number of rows in the table and the number of rows that have been modified since the last collection of statistics. 
- The column-level statistics information is more abundant, including histograms, Count-Min Sketch, Top-N (values or indexes with the highest occurrences), distribution and quantity of different values, and the number of null values, and so on.

To ensure the statistics are healthy and representative, you can use the following commands:

1. `SHOW STATS_META`: This command provides metadata about table statistics.
2. `SHOW STATS_HEALTHY`: This command shows the health status of table statistics.

For example, you can use:

```SQL
tidb> SHOW STATS_META WHERE table_name='T2'\G;
*************************** 1. row ***************************
 Db_name: test
 Table_name: T2
Partition_name:
 Update_time: 2023-05-11 02:16:50
 Modify_count: 20000
 Row_count: 20000
1 row in set (0.03 sec)

tidb> SHOW STATS_HEALTHY WHERE table_name='T2'\G;
*************************** 1. row ***************************
    Db_name: test
  Table_name: T2
Partition_name:
    Healthy: 0
1 row in set (0.00 sec)

```

In TiDB database, there are two ways to collect statistics: automatic collection and manual collection. In most case, the auto collection job works fine. Automatic collection is actually triggered when certain conditions are met for a table, and TiDB will automatically collect statistics. We commonly use three triggering conditions, which are: ratio, start_time and end_time.
tidb_auto_analyze_ratio: The healthiness trigger
tidb_auto_analyze_start_time and tidb_auto_analyze_end_time: The allowed job window
```SQL
mysql> show variables like '%auto_analyze%';
+-----------------------------------------+-------------+
| Variable_name                           | Value       |
+-----------------------------------------+-------------+
| tidb_auto_analyze_ratio                 | 0.5         |
| tidb_auto_analyze_start_time            | 00:00 +0000 |
| tidb_auto_analyze_end_time              | 23:59 +0000 |
+-----------------------------------------+-------------+
```

In cases where automatic collection doesn't meet your needs, you can manually collect statistics using the `ANALYZE TABLE table_name` statement. This allows you to:

1. Adjust the sample rate for more accurate or faster analysis
2. Increase the number of top-N values collected
3. Gather statistics for specific columns only

It's important to note that after manual collection, subsequent automatic gathering jobs will inherit the new parameters. This means that any customizations you've made during manual collection will be carried forward in future automatic analyses.

Another common scenario is locking table statistics. This is useful when:
1. The statistics on the table are already representative of the data.
2. The table is very large and statistics collection is time-consuming.
3. You want to maintain statistics only during specific time windows.

To lock the statistics for a table, you can use the following command `LOCK STATS table_name`.

for more detail about statistics, please refer to [statistics](https://docs.pingcap.com/tidb/stable/statistics).

## How TiDB build A Execution Plan
An SQL statement undergoes optimization primarily in the optimizer through three stages:
- Pre-Processing
- Logical Transformation
- Cost-based Optimization

### Pre-Processing
The main actions in the pre-processing stage it to determine if the SQL statement can be executed by using Point_Get or Batch_Point_Get.

Point_Get or Batch_Point_Get is to get 1 or 0 or many row only by using the TiKV key, the explicit or implicit (_tidb_rowid) primary key. For example, when id column is the primary key of a clustered index table, Point_Get is used to get the particular row. If a plan is identified as Point_Get, optimizer will skip the logical transformation and cost-based optimization.

```SQL
SELECT id, name FROM emp WHERE id = 901; 
```

### Logical Transformation

The purpose of logical Transformation is to optimize the execution of statements based on the characteristics of SELECT list, WHERE predicates, and other predicates in SQL queries. It generates a logical execution plan to annotate and rewrite the query. This logical plan is then passed to the Cost-Based Optimization. The optimization rules include column pruning，partition pruning，eliminate Max/Min, eliminate outer join, join reorder, predicates push-down，subquery rewrite，derive TopN from window functions，and de-correlation of correlated Subquery. Since this step is fully automated by the query optimizer, it usually does not require manual adjustments.

More Detail for Logical Transformation: https://docs.pingcap.com/tidb/stable/sql-logical-optimization.

### Cost-Based Optimization

TiDB uses statistics as input to the optimizer to estimate the number of rows processed in each plan step for a SQL statement. The Cost-Based Optimization estimates the cost of each available plan choice, including index accesses and the sequence of table joins, and produces a cost for each available plan. The optimizer then picks the execution plan with the lowest overall cost.


The below figure illustrates the various data access paths and row set operations that cost-based optimization can consider to develop the optimal execution plan. Furthermore, during the logical transformation stage, the query has already been rewritten for predicate push-down. In the cost-based optimization stage, TiKV expression push-down is further implemented when possible at TiKV layer. 

Furthermore, confirming the algorithm for certain SQL operations, such as aggregation, join, and sorting, is essential. For instance, the aggregation operator may utilize either HASH_AGG or STREAM_AGG, while the join operator can select from HASH JOIN, MERGE JOIN, or INDEX JOIN. Likewise, various options are available for the sorting operator.

![cost-based-optimization](/media/sql-tuning/cost-based-optimization.png)

# 3. Understanding Execution Plans
The execution plan represents the steps TiDB will follow to execute a SQL query. An effective execution plan ensures performance optimization.


# 4. Real-World Use Cases

## Debugging a Bad Plan
When a SQL query performs poorly, it’s usually due to an inefficient execution plan. Key steps to debug:

Compare estimated rows and actual rows from the execution plan. Large discrepancies indicate a problem with the statistics or index selection.
Ensure appropriate indexes are being used. If the query scans large portions of data, it may require better indexing or statistics updates.

## Index Strategy in TiDB
Indexes play a critical role in query performance. A well-designed index can drastically reduce query execution time.

Make sure to follow the composite index strategy
- Key predicates for index prefix columns, predicates on columns that data can be accessed directly: 
- Columns After Index Prefix, leverage index to preprocess the sorting
- Additional Columns for Filtering, to reduce row lookups

## Partition Tables: Local vs Global Indexes
Partitioning large tables can improve query performance, but the choice between local and global indexes is crucial.

- Local Index: Indexed data is limited to a partition, leading to efficient access within that partition.
- Global Index: Spanning across partitions, global indexes help avoid partition lookups.
Best Practice: Use global indexes for queries accessing multiple partitions to reduce lookup overhead.

## Index Full Scan vs Table Full Scan
In most cases, Index Full Scans are much faster than Table Full Scans because indexes are smaller and ordered. Use index full scans to minimize data access time.

Best Practice:

Ensure that queries make use of index scans whenever possible. A full table scan should only happen when absolutely necessary.


# Key Takeaways
- The quality of the execution plan is dependent on the quality of the inputs. Ensure statistics are up to date and reflect the data accurately.
- Compare estimated rows and actual rows to verify that the optimizer has accurate statistics.
- Follow the composite index strategy:
  - Use key predicates for index prefix columns.
  - Leverage columns after the index prefix to support sorting and filtering.
 . - Add extra columns to reduce row lookups.
- Choose between global and local indexes for partitioned tables based on the query access pattern. Use global indexes to avoid lookup overhead across partitions.

